{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#load cifar100, display a random image\n",
    "#(train_data, val_data, test_data), metadata = tfds.load(\n",
    "#    'cifar10',\n",
    "#    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "#    with_info=True,\n",
    "#    #as_supervised=True,\n",
    "#)\n",
    "#num_classes = metadata.features['label'].num_classes\n",
    "#print(num_classes)\n",
    "\n",
    "#get_label_name = metadata.features['label'].int2str\n",
    "\n",
    "\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(train_data, val_data, test_data), info = tfds.load('cifar10', split=['train[:80%]', 'train[80%:]', 'test'], \n",
    "                                             with_info=True, as_supervised=True)\n",
    "\n",
    "# Define preprocessing function\n",
    "IMG_SIZE = 32\n",
    "\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 255.0)\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    return image, label\n",
    "\n",
    "# Apply preprocessing function to datasets\n",
    "train_data = train_data.map(preprocess_image)\n",
    "val_data = val_data.map(preprocess_image)\n",
    "test_data = test_data.map(preprocess_image)\n",
    "\n",
    "# Shuffle and batch datasets\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data = train_data.shuffle(buffer_size=10000)\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "val_data = val_data.batch(BATCH_SIZE)\n",
    "\n",
    "test_data = test_data.batch(BATCH_SIZE,drop_remainder=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#train_data=train_data.batch(32)\n",
    "#val_data=val_data.batch(32)\n",
    "#test_data=test_data.batch(32,drop_remainder=True)\n",
    "\n",
    "#resize and rescale images - redefine since we are using batches of images now\n",
    "IMG_SIZE = 32\n",
    "\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.1),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "    ]\n",
    ")\n",
    "def build_model(dropout_rate=0.2):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(32, 32, 3)),\n",
    "        data_augmentation,\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    # Train the model\n",
    "    history = model.fit(train_data, epochs=10, validation_data=val_data, batch_size=32)\n",
    "\n",
    "    # Save the model\n",
    "    model.save('models/CNNCIFAR10.h5')\n",
    "    return model\n",
    "\n",
    "if not os.path.exists('models/CNNCIFAR10.h5'):\n",
    "    model=build_model()\n",
    "else:\n",
    "    model=tf.keras.models.load_model('models/CNNCIFAR10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 4s 11ms/step - loss: 0.9553 - accuracy: 0.6730\n",
      "Test loss: 0.9553128480911255\n",
      "Test accuracy: 0.6729767918586731\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_data) #since we specified metrics=[accuracy], evaluate() returns test_acc too.\n",
    "                                                #dont pass labels since test_data contains them as it is a dataset\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]], shape=(1, 10), dtype=float32)\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [21:55<00:00, 131.50s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd7dd5f75b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd7dd5f75b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 106ms/step\n",
      "Original prediction: 3\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Prediction with patch: 1\n",
      "Probability of patch working: 0.8914563301282051\n",
      "tf.Tensor(\n",
      "[[[0. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 1. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 1. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 1. 0.]\n",
      "  [1. 1. 1.]\n",
      "  [0. 1. 1.]\n",
      "  ...\n",
      "  [1. 0. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [0. 1. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 1. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 1. 0.]]], shape=(24, 24, 3), dtype=float32)\n",
      "[[[0. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 1. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 1. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 1. 0.]\n",
      "  [1. 1. 1.]\n",
      "  [0. 1. 1.]\n",
      "  ...\n",
      "  [1. 0. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [0. 1. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 1. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 1. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "#test_data=test_data.shuffle(1000).take(1)\n",
    "#for i, (image, labels) in enumerate(test_data):\n",
    "#  img=image[0, ...]\n",
    "#  label=labels[0]\n",
    "#print(label)\n",
    "#for images in test_data:\n",
    "\n",
    "#patch_exists = tf.io.gfile.exists(\"patches/patch_20x20_cifar10\")\n",
    "if not os.path.exists('patches/patch_24x24_cifar10.npy'):\n",
    "    #do below\n",
    "    # Define the target class (toaster)\n",
    "    #target_class = 1 95 in cifar100\n",
    "    #indices=[0,0,0,0,0,0,0,0,0,0]\n",
    "    indices=[9] #target_class=10\n",
    "    target_class=tf.one_hot(indices,depth=10)\n",
    "    print(target_class)\n",
    "    #TODO: patch with random size (% of image) - max % of image will be 50% as shown in study\n",
    "    patch_size=0.5 #end up with patch_size=height(=width)(=diameter)\n",
    "    total_img=IMG_SIZE*IMG_SIZE*patch_size\n",
    "    patch_size=round(math.sqrt(total_img))\n",
    "    if (patch_size%2!=0):\n",
    "        patch_size=patch_size+1\n",
    "    half_size=patch_size/2 #distance from edge to centre of patch(=radius)\n",
    "    half_size=int(half_size)\n",
    "    #TODO: circular patch (and applying random rotations)\n",
    "    # Generate the adversarial patch\n",
    "    #patch_size = 20\n",
    "    print(patch_size)\n",
    "\n",
    "    #print(indices)\n",
    "    #patch_value = tf.Variable(initial_value=tf.ones((patch_size, patch_size, 3)),dtype=tf.float32)\n",
    "    patch_value = np.ones((patch_size, patch_size, 3))\n",
    "    #patch_value = tf.ones((patch_size, patch_size, 3))\n",
    "    #indices = tf.constant([[i, j] for i in range(patch_value[0], patch_value[0]+patch_size) for j in range(patch_value[1], patch_value[1]+patch_size)])\n",
    "    learning_rate = 10\n",
    "    num_iterations = 10\n",
    "    patch_value=tf.convert_to_tensor(patch_value, dtype=tf.float32)\n",
    "    #ran_x=random.randint(11,20)\n",
    "    #ran_y=random.randint(11,20)\n",
    "    def apply_patch2(original_image,patch,x,y):\n",
    "        #TODO:insert patch with center x,y to image\n",
    "        ##identity:tensor 32x32=1 so img*i=img\n",
    "        #ident=tf.ones(IMG_SIZE,IMG_SIZE,3)\n",
    "        #first set all pixels in the image where the patch would go to 0 - mask\n",
    "        mask=\n",
    "        #then expand the dimensions of the patch s.t. it is the same shape as the image but with 0's everywhere else other than the patch\n",
    "        #then add them\n",
    "\n",
    "    def apply_patch(original_image,patch,x,y):\n",
    "        #apply patch to img with center x,y\n",
    "        #find out how many rows are above the patch and columns to the left of the patch:\n",
    "        #center can only be between 12,12 and 21,21\n",
    "        #get rows above patch\n",
    "        #get rows inline with patch\n",
    "        #get rows below patch\n",
    "        #concatenate all downwards\n",
    "        #print(x,y)\n",
    "        above=tf.concat([original_image[:x-half_size,:y-half_size,:],original_image[x-half_size:,:y-half_size,:]],axis=0)\n",
    "        inl=tf.concat([original_image[:x-half_size,y-half_size:y+half_size,:],patch,original_image[x+half_size:,y-half_size:y+half_size,:]],axis=0)\n",
    "        below=tf.concat([original_image[:x-half_size,y+half_size:,:],original_image[x-half_size:,y+half_size:,:]],axis=0)\n",
    "        #print(below)\n",
    "        patched_image=tf.concat([above,inl,below],axis=1)\n",
    "\n",
    "        return patched_image\n",
    "    def loss_func(pred):\n",
    "        loss_per_example = tf.keras.losses.categorical_crossentropy(y_true=target_class, y_pred=pred, from_logits=False, label_smoothing=0)\n",
    "        loss = tf.reduce_mean(loss_per_example)\n",
    "        return loss\n",
    "\n",
    "    #TODO: use trange for progress bar\n",
    "    counter=0\n",
    "    patch_misclassified=0\n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        for batch in test_data:\n",
    "            images,labels=batch\n",
    "            for j in range(images.shape[0]):\n",
    "                counter=counter+1\n",
    "                img=images[j,...]\n",
    "                label=labels[j]\n",
    "                #plt.imshow(img)\n",
    "                #plt.show()\n",
    "                #TODO: allow partially visible patch? i.e only half the patch is shown, so only train the portion shown\n",
    "                ran_x=random.randint(half_size+1,32-half_size-1)\n",
    "                ran_y=random.randint(half_size+1,32-half_size-1)\n",
    "                #place patch in random place\n",
    "                \n",
    "                #get the predictions for that image\n",
    "                #get loss for that image\n",
    "                #update patch\n",
    "                #next image -> put patch in another random spot\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tape.watch(patch_value)\n",
    "                    #print(img)\n",
    "                    img_patch = np.copy(img)\n",
    "                    #patch_value = patch_value.numpy()\n",
    "                    img_patch = tf.convert_to_tensor(img_patch, dtype=tf.float32)\n",
    "                    img_patch=apply_patch(img_patch,patch_value,ran_x,ran_y)\n",
    "                    predictions = model(img_patch[np.newaxis, ...])\n",
    "                    #loss = -predictions[0][target_class]\n",
    "                    #print('original:',tf.get_static_value(label))\n",
    "                    #print('Prediction with patch:', np.argmax(predictions))\n",
    "                    if (tf.get_static_value(label)!=np.argmax(predictions)):\n",
    "                        patch_misclassified=patch_misclassified+1\n",
    "                    loss = loss_func(predictions)\n",
    "                gradients = tape.gradient(loss, patch_value)\n",
    "                #patch_value = patch_value - learning_rate * gradients\n",
    "                #tf.keras.optimizers.Adam().apply_gradients([(gradients,patch_value)])\n",
    "\n",
    "                #projected gradient descent\n",
    "                gradients = tf.sign(gradients) * learning_rate\n",
    "                patch_value = patch_value + tf.squeeze(gradients)\n",
    "                patch_value = tf.clip_by_value(patch_value, 0, 1)\n",
    "\n",
    "\n",
    "    # Apply the adversarial patch to the image\n",
    "    img_patch = np.copy(img)\n",
    "    img_patch = tf.convert_to_tensor(img_patch, dtype=tf.float32)\n",
    "    patched_image=apply_patch(img_patch,patch_value,ran_x,ran_y)\n",
    "\n",
    "    plt.imshow(patched_image.numpy())\n",
    "    plt.show()\n",
    "\n",
    "    # Check the model's prediction on the original image\n",
    "    prediction_orig = model.predict(img[np.newaxis, ...])\n",
    "    print('Original prediction:', np.argmax(prediction_orig))\n",
    "\n",
    "    # Check the model's prediction on the image with the adversarial patch\n",
    "    prediction_patch = model.predict(patched_image[np.newaxis, ...])\n",
    "    print('Prediction with patch:', np.argmax(prediction_patch))\n",
    "\n",
    "    print('Probability of patch working:',patch_misclassified/counter)\n",
    "    #TODO: run on computer using gpu\n",
    "    #tf.io.write_file(\"patches/patch_20x20_cifar10\",patch_value)\n",
    "    #print(patch_value)\n",
    "    #print(patch_value.numpy())\n",
    "    np.save('patches/patch_24x24_cifar10.npy',patch_value.numpy()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 1. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 1. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 1. 0.]\n",
      "  [1. 1. 1.]\n",
      "  [0. 1. 1.]\n",
      "  ...\n",
      "  [1. 0. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [0. 1. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 1. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 1. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 1. 0.]]], shape=(24, 24, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "patch_value=np.load(\"patches/patch_24x24_cifar10.npy\")\n",
    "patch_value=tf.convert_to_tensor(patch_value, dtype=tf.float32)\n",
    "print(patch_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1643926eecc1b23d34b88c1a83f4ec8603ca2a6e16e6546068c9af1ba4a8f649"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
